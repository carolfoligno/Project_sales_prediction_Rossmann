{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "624b8470",
   "metadata": {},
   "source": [
    "# Previsão de Vendas das Lojas Rossmann\n",
    "\n",
    "### Este projeto foi orientado pela Comunidade DS, utilizando os dados disponíveis no Kaggle da Rede de Lojas Rossmann. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e169f872",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Importação de Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c6537b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:36:59.032327Z",
     "start_time": "2022-11-30T17:36:57.665159Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import inflection\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import pickle\n",
    "import warnings\n",
    "\n",
    "\n",
    "from scipy import stats as ss\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3e746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:36:59.042685Z",
     "start_time": "2022-11-30T17:36:59.033799Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(x_training, kfold, model_name, model, verbose=False):\n",
    "    mae_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    \n",
    "    for k in reversed(range(1, kfold +1 )):\n",
    "        if verbose:\n",
    "            print(f'\\nKFold Number: {k}')\n",
    "            \n",
    "        # start and end date for validation\n",
    "        validation_start_date = x_training['date'].max() - datetime.timedelta(days=k*6*7)\n",
    "        validation_end_date = x_training['date'].max() - datetime.timedelta(days=(k-1)*6*7)\n",
    "\n",
    "        # filtering dataset\n",
    "        training = x_training[x_training['date'] < validation_start_date]\n",
    "        validation = x_training[(x_training['date'] >= validation_start_date) &\n",
    "                                (x_training['date'] <= validation_end_date)]\n",
    "\n",
    "        # training and validation dataset\n",
    "        # training \n",
    "        xtraining = training.drop(['date', 'sales'], axis=1)\n",
    "        ytraining = training['sales']\n",
    "\n",
    "        # validation\n",
    "        xvalidation = validation.drop(['date', 'sales'], axis=1)\n",
    "        yvalidation = validation['sales']\n",
    "\n",
    "        # model\n",
    "        m = model.fit(xtraining, ytraining)\n",
    "\n",
    "        # prediction\n",
    "        yhat = m.predict(xvalidation)\n",
    "\n",
    "        # performance\n",
    "        m_result = ml_error(model_name, np.expm1(yvalidation), np.expm1(yhat))\n",
    "\n",
    "        # store performance of each kfold iteration\n",
    "        mae_list.append(m_result['MAE'])\n",
    "        mape_list.append(m_result['MAPE'])\n",
    "        rmse_list.append(m_result['RMSE'])\n",
    "\n",
    "        # dataframe\n",
    "        return pd.DataFrame({'Model Name': model_name,\n",
    "                        'MAE CV': np.round(np.mean(mae_list),2 ).astype(str) + ' +/- ' + np.round(np.std(mae_list),2 ).astype(str),\n",
    "                        'MAPE CV': np.round(np.mean(mape_list),2 ).astype(str) + ' +/- ' + np.round(np.std(mape_list),2 ).astype(str),\n",
    "                        'RMSE CV': np.round(np.mean(rmse_list),2 ).astype(str) + ' +/- ' + np.round(np.std(rmse_list),2 ).astype(str)}, index=[0])\n",
    "\n",
    "def mean_absolute_percentage_error(y, yhat):\n",
    "    return np.mean(np.abs( ( y - yhat )/ y ) )\n",
    "\n",
    "def ml_error(model_name, y, yhat):\n",
    "    mae = mean_absolute_error(y, yhat)\n",
    "    mape = mean_absolute_percentage_error(y, yhat)\n",
    "    rmse = np.sqrt(mean_squared_error(y, yhat))\n",
    "    \n",
    "    return pd.DataFrame({'Model Name': model_name,\n",
    "                        'MAE': mae,\n",
    "                        'MAPE': mape,\n",
    "                        'RMSE': rmse}, index=[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b422cd0",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ff2a2",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Carregando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd954e8f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T19:03:50.541117Z",
     "start_time": "2022-11-27T19:03:49.957798Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# low_memory = False (ler todo o arquivo na mesma hora coloca na memoria)\n",
    "df_sales_raw = pd.read_csv ('data/train.csv', low_memory=False)\n",
    "df_store_raw = pd.read_csv ('data/store.csv', low_memory=False)\n",
    "\n",
    "# merge\n",
    "df_raw = pd.merge(df_sales_raw, df_store_raw, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21d1559",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121dccc3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:49:46.174426Z",
     "start_time": "2022-11-25T17:49:45.976649Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1 = df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9df95f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:49:46.180611Z",
     "start_time": "2022-11-25T17:49:46.176959Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_old = ['Store', 'DayOfWeek', 'Date', 'Sales', 'Customers', 'Open', 'Promo',\n",
    "       'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "       'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "       'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "       'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "cols_new = list(map(snakecase, cols_old))\n",
    "\n",
    "# rename columns\n",
    "df1.columns = cols_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4716b555",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:49:46.269270Z",
     "start_time": "2022-11-25T17:49:46.182191Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mudar o types da coluna date\n",
    "df1['date'] = pd.to_datetime(df1['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953d5f0",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Tratamento de valores inconsistentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af728948",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5bafc856",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Tratamento de valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a71ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:15.398444Z",
     "start_time": "2022-11-25T17:49:46.270915Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#competition_distance - distancia em metros da loja competidora proxima\n",
    "    # uma das formas de eliminar os NAs é preenchendo as linhas faltantes\n",
    "    # utilizando a lógica de que talvez  NA é pq a loja competidora está bem distante\n",
    "    # e para isso irei preencher um valor qualquer 200000.0 (maior q o valor maximo do meu dataframe) \n",
    "    \n",
    "df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "\n",
    "#competition_open_since_month - mes/ano que a loja competidora foi aberta\n",
    "# quando é mais de uma coluna coloca axis=1\n",
    "# foi preenchido de acordo com a data de registro\n",
    "    \n",
    "df1['competition_open_since_month']= df1.apply(lambda x: x['date'].month \n",
    "                                               if math.isnan(x['competition_open_since_month']) else \n",
    "                                               x['competition_open_since_month'], axis=1)\n",
    "\n",
    "#competition_open_since_year  \n",
    "df1['competition_open_since_year']= df1.apply(lambda x: x['date'].year \n",
    "                                               if math.isnan(x['competition_open_since_year']) else      \n",
    "                                               x['competition_open_since_year'], axis=1)                       \n",
    "#promo2_since_week\n",
    "df1['promo2_since_week']= df1.apply(lambda x: x['date'].week \n",
    "                                               if math.isnan(x['promo2_since_week']) else      \n",
    "                                               x['promo2_since_week'], axis=1)  \n",
    "#promo2_since_year   \n",
    "df1['promo2_since_year']= df1.apply(lambda x: x['date'].year \n",
    "                                               if math.isnan(x['promo2_since_year']) else      \n",
    "                                               x['promo2_since_year'], axis=1)\n",
    "\n",
    "\n",
    "df1['promo_interval'] = df1['promo_interval'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cebd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:15.420755Z",
     "start_time": "2022-11-25T17:50:15.399674Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "\n",
    "df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c718f747",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7d6eaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:15.736099Z",
     "start_time": "2022-11-25T17:50:15.422170Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2 = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2edda85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:44.319606Z",
     "start_time": "2022-11-25T17:50:15.737767Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## extraindo novas features da coluna date\n",
    "# year\n",
    "df2['year'] = df2['date'].dt.year\n",
    "\n",
    "# month\n",
    "df2['month'] = df2['date'].dt.month\n",
    "\n",
    "# day\n",
    "df2['day'] = df2['date'].dt.day\n",
    "\n",
    "# week of year\n",
    "df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "\n",
    "# year week\n",
    "df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "## features de competition\n",
    "# competition since - juntas o mes - ano\n",
    "df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year= x['competition_open_since_year'], month= x['competition_open_since_month'] , day=1), axis=1)\n",
    "df2['competition_time_month'] = ((df2['date'] - df2['competition_since'])/30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "## feature de promo\n",
    "# promo since\n",
    "df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "# transformando em datetime\n",
    "df2['promo_since'] = df2['promo_since'].apply(lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w') - datetime.timedelta(days=7))\n",
    "df2['promo_time_week'] = ((df2['date'] - df2['promo_since'])/7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "## feature assortment\n",
    "df2['assortment']= df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "# state holiday\n",
    "df2['state_holiday'] = df2['state_holiday'].apply(lambda x:'public_holiday' if x == 'a' else 'easter_holiday' if x == 'b' else 'christmas' if x == 'c' else 'regular_day') \n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------\n",
    "\n",
    "#promo_interval - intervalos consecutivos quando a promo2 foi iniciada\n",
    "# dicionario indicando o numero dos meses\n",
    "month_map = {1: 'Jan',\n",
    "            2: 'Feb',\n",
    "            3: 'Mar',\n",
    "            4: 'Apr',\n",
    "            5: 'May',\n",
    "            6: 'Jun',\n",
    "            7: 'Jul',\n",
    "            8: 'Aug',\n",
    "            9: 'Sept',\n",
    "            10: 'Oct',\n",
    "            11: 'Nov',\n",
    "            12: 'Dec'}\n",
    "\n",
    "    # assumption: criadno uma coluna onde indica o mes da coluna date     \n",
    "df2['month_map'] = df2['date'].dt.month.map(month_map)\n",
    "\n",
    "    # assumption: se o mes estiver presente na coluna 'promo_interval' logo tem promocao ativa naquela data \n",
    "df2['is_promo'] = df2[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval']==0 else \n",
    "                                                            1 if x['month_map'] in x['promo_interval'].split(',') \n",
    "                                                            else 0, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8138174b",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Seleção e Filtragem do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ad3a98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:44.672241Z",
     "start_time": "2022-11-25T17:50:44.320963Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a455bf27",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:44.781875Z",
     "start_time": "2022-11-25T17:50:44.673492Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# as restrições observadas para o negócio foram loja fechada e vendas nulas\n",
    "df3 = df3[(df3['open'] != 0) & (df3['sales'] != 0 )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5c04b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:44.864514Z",
     "start_time": "2022-11-25T17:50:44.783123Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# a coluna customers é uma restrição para o modelo, pois tem essa informação daqui 6 semanas\n",
    "# a não ser que eu faça outro modelo de previsão para calcular a coluna customers daqui 6 semanas.\n",
    "# então por isso, foi retirar essa coluna do meu dataset para fazer a predição de vendas daqui 6 semanas.\n",
    "cols_drop = ['customers', 'open', 'promo_interval', 'month_map']\n",
    "df3 = df3.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38babbde",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21061eb6",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Divisão entre previsores e classes / Treinamento e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3431587d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:44.910007Z",
     "start_time": "2022-11-25T17:50:44.865956Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df4 = df3.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46542324",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.156197Z",
     "start_time": "2022-11-25T17:50:44.911370Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# pegar as ultimas 6 semanas e fazer como teste\n",
    "# data antes das ultimas 6 semanas será os meus dados de treino\n",
    "\n",
    "# para descobrir a data antes das 6 semanas\n",
    "#df4[['store', 'date']].groupby('store').max().reset_index()['date'][0] - datetime.timedelta( days=6*7 ) # resultado 2015-06-19\n",
    "\n",
    "# training dataset\n",
    "X_train = df4[df4['date'] < '2015-06-19']\n",
    "y_train = X_train['sales']\n",
    "\n",
    "# test dataset\n",
    "X_test = df4[df4['date'] >= '2015-06-19']\n",
    "y_test = X_test['sales']\n",
    "\n",
    "print('Training Min Date: {}'.format(X_train['date'].min()))\n",
    "print('Training Max Date: {}'.format(X_train['date'].max()))\n",
    "\n",
    "print('\\nTest Min Date: {}'.format(X_test['date'].min()))\n",
    "print('Test Min Date: {}'.format(X_test['date'].max()))\n",
    "\n",
    "\n",
    "# aqui, eu separei o meu dataset entre o de treino (data menor que 2015-06-19)\n",
    "# e dataset de teste (data maior e igual que 2015-06-19)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f28460",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Data preparation - Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c999d461",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.159333Z",
     "start_time": "2022-11-25T17:50:45.157466Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Primeira motificação: mudar os dados categoricos para numericos\n",
    "# Segunda modificação: em relação ao range, colocar todos na mesma escala."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82288dad",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Normalização**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00430d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.165010Z",
     "start_time": "2022-11-25T17:50:45.160525Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# buscar uma variavel com distribuição normal\n",
    "# como vimos no Numerical Variable não há uma variavel com distribuição normal\n",
    "# optamos em não normalizar nenhuma variavel, pois pode afetar o meu algoritmo forçando essa normalização. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b1311",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Rescaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a295083",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.168675Z",
     "start_time": "2022-11-25T17:50:45.166173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# não muda a natureza da variavel\n",
    "# min-max scaler é sensivel aos outliers\n",
    "# robust scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cee54a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.552698Z",
     "start_time": "2022-11-25T17:50:45.169884Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# competition distance\n",
    "X_train['competition_distance'] = rs.fit_transform( X_train[['competition_distance']].values )\n",
    "pickle.dump(rs, open('parameter/competition_distance_scaler.pkl', 'wb'))\n",
    "\n",
    "# competition time month\n",
    "X_train['competition_time_month'] = rs.fit_transform( X_train[['competition_time_month']].values )\n",
    "pickle.dump(rs, open('parameter/competition_time_month_scaler.pkl', 'wb'))\n",
    "\n",
    "# promo time week\n",
    "X_train['promo_time_week'] = mms.fit_transform( X_train[['promo_time_week']].values )\n",
    "pickle.dump(mms, open('parameter/promo_time_week_scaler.pkl', 'wb'))\n",
    "\n",
    "# year\n",
    "X_train['year'] = mms.fit_transform( X_train[['year']].values )\n",
    "pickle.dump(mms, open('parameter/year_scaler.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98613c34",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Transformação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071a39f9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Enconding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d90289",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.555764Z",
     "start_time": "2022-11-25T17:50:45.553958Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# enconding - muda a variavel de categorica para numerica sem mudar o conteudo de unformação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d78cb25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.898898Z",
     "start_time": "2022-11-25T17:50:45.557564Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# state_holiday - One Hot Encoding\n",
    "X_train = pd.get_dummies(X_train, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "# store_type - Label Enconding\n",
    "le = LabelEncoder()\n",
    "X_train['store_type'] = le.fit_transform(X_train['store_type'])\n",
    "pickle.dump(le, open('parameter/store_type_scaler.pkl', 'wb'))\n",
    "\n",
    "# assortment - Ordinal Enconding\n",
    "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "X_train['assortment'] = X_train['assortment'].map(assortment_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc1a17c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Transformação da Variável Alvo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf5663e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.911081Z",
     "start_time": "2022-11-25T17:50:45.900246Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# transformação logaritma \n",
    "X_train['sales'] = np.log1p(X_train['sales'])\n",
    "\n",
    "y_train = np.log1p(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b0ef81",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Transformação de Natureza**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462e5c22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:45.932376Z",
     "start_time": "2022-11-25T17:50:45.915388Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5f3c7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:51.685589Z",
     "start_time": "2022-11-25T17:50:45.933718Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "## natureza ciclica - seno e cosseno\n",
    "# day of week\n",
    "X_train['day_of_week_sin'] = X_train['day_of_week'].apply(lambda x: np.sin(x * ( 2. * np.pi/7 )))\n",
    "X_train['day_of_week_cos'] = X_train['day_of_week'].apply(lambda x: np.cos(x * ( 2. * np.pi/7 )))\n",
    "\n",
    "# month\n",
    "X_train['month_sin'] = X_train['month'].apply(lambda x: np.sin(x * ( 2. * np.pi/12 )))\n",
    "X_train['month_cos'] = X_train['month'].apply(lambda x: np.cos(x * ( 2. * np.pi/12 )))\n",
    "\n",
    "# day\n",
    "X_train['day_sin'] = X_train['day'].apply(lambda x: np.sin(x * ( 2. * np.pi/30 )))\n",
    "X_train['day_cos'] = X_train['day'].apply(lambda x: np.cos(x * ( 2. * np.pi/30 )))\n",
    "\n",
    "# week of year\n",
    "X_train['week_of_year_sin'] = X_train['week_of_year'].apply(lambda x: np.sin(x * ( 2. * np.pi/52 )))\n",
    "X_train['week_of_year_cos'] = X_train['week_of_year'].apply(lambda x: np.cos(x * ( 2. * np.pi/52 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0ae8aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:51.834896Z",
     "start_time": "2022-11-25T17:50:51.686969Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop a coluna original que deriva a coluna_sin e coluna_cos\n",
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\n",
    "X_train = X_train.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806c809",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:52.137906Z",
     "start_time": "2022-11-25T17:50:51.836121Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SALVANDO \n",
    "with open('parameter/train_prepare.pkl', mode = 'wb') as f:\n",
    "    pickle.dump([X_train, y_train], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9130f092",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Data preparation - Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc925f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:52.463403Z",
     "start_time": "2022-11-25T17:50:52.141002Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Primeira motificação: mudar os dados categoricos para numericos\n",
    "# Segunda modificação: em relação ao range, colocar todos na mesma escala.\n",
    "\n",
    "# **Normalização**\n",
    "\n",
    "# buscar uma variavel com distribuição normal\n",
    "# como vimos no Numerical Variable não há uma variavel com distribuição normal\n",
    "# optamos em não normalizar nenhuma variavel, pois pode afetar o meu algoritmo forçando essa normalização. \n",
    "\n",
    "# **Rescaling**\n",
    "\n",
    "# não muda a natureza da variavel\n",
    "# min-max scaler é sensivel aos outliers\n",
    "# robust scaler\n",
    "\n",
    "rs = RobustScaler()\n",
    "mms = MinMaxScaler()\n",
    "\n",
    "# competition distance\n",
    "X_test['competition_distance'] = rs.fit_transform( X_test[['competition_distance']].values )\n",
    "# pickle.dump(rs, open('/home/caroline/repos/DataScience_em_Producao/parameter/competition_distance_scaler.pkl', 'wb'))\n",
    "\n",
    "# competition time month\n",
    "X_test['competition_time_month'] = rs.fit_transform( X_test[['competition_time_month']].values )\n",
    "# pickle.dump(rs, open('/home/caroline/repos/DataScience_em_Producao/parameter/competition_time_month_scaler.pkl', 'wb'))\n",
    "\n",
    "# promo time week\n",
    "X_test['promo_time_week'] = mms.fit_transform( X_test[['promo_time_week']].values )\n",
    "# pickle.dump(mms, open('/home/caroline/repos/DataScience_em_Producao/parameter/promo_time_week_scaler.pkl', 'wb'))\n",
    "\n",
    "# year\n",
    "X_test['year'] = mms.fit_transform( X_test[['year']].values )\n",
    "# pickle.dump(mms, open('/home/caroline/repos/DataScience_em_Producao/parameter/year_scaler.pkl', 'wb'))\n",
    "\n",
    "##### Transformação\n",
    "\n",
    "# **Enconding**\n",
    "\n",
    "# enconding - muda a variavel de categorica para numerica sem mudar o conteudo de unformação\n",
    "\n",
    "# state_holiday - One Hot Encoding\n",
    "X_test = pd.get_dummies(X_test, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "# store_type - Label Enconding\n",
    "le = LabelEncoder()\n",
    "X_test['store_type'] = le.fit_transform(X_test['store_type'])\n",
    "# pickle.dump(le, open('/home/caroline/repos/DataScience_em_Producao/parameter/store_type_scaler.pkl', 'wb'))\n",
    "\n",
    "# assortment - Ordinal Enconding\n",
    "assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "X_test['assortment'] = X_test['assortment'].map(assortment_dict)\n",
    "\n",
    "# **Transformação da Variável Alvo**\n",
    "\n",
    "# transformação logaritma \n",
    "X_test['sales'] = np.log1p(X_test['sales'])\n",
    "\n",
    "y_test = np.log1p(y_test)\n",
    "\n",
    "\n",
    "# **Transformação de Natureza**\n",
    "\n",
    "## natureza ciclica - seno e cosseno\n",
    "# day of week\n",
    "X_test['day_of_week_sin'] = X_test['day_of_week'].apply(lambda x: np.sin(x * ( 2. * np.pi/7 )))\n",
    "X_test['day_of_week_cos'] = X_test['day_of_week'].apply(lambda x: np.cos(x * ( 2. * np.pi/7 )))\n",
    "\n",
    "# month\n",
    "X_test['month_sin'] = X_test['month'].apply(lambda x: np.sin(x * ( 2. * np.pi/12 )))\n",
    "X_test['month_cos'] = X_test['month'].apply(lambda x: np.cos(x * ( 2. * np.pi/12 )))\n",
    "\n",
    "# day\n",
    "X_test['day_sin'] = X_test['day'].apply(lambda x: np.sin(x * ( 2. * np.pi/30 )))\n",
    "X_test['day_cos'] = X_test['day'].apply(lambda x: np.cos(x * ( 2. * np.pi/30 )))\n",
    "\n",
    "# week of year\n",
    "X_test['week_of_year_sin'] = X_test['week_of_year'].apply(lambda x: np.sin(x * ( 2. * np.pi/52 )))\n",
    "X_test['week_of_year_cos'] = X_test['week_of_year'].apply(lambda x: np.cos(x * ( 2. * np.pi/52 )))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb007ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:52.470273Z",
     "start_time": "2022-11-25T17:50:52.464712Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop a coluna original que deriva a coluna_sin e coluna_cos\n",
    "cols_drop = ['week_of_year', 'day', 'month', 'day_of_week', 'promo_since', 'competition_since', 'year_week']\n",
    "X_test = X_test.drop(cols_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46e22cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:52.485557Z",
     "start_time": "2022-11-25T17:50:52.471450Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# SALVANDO\n",
    "\n",
    "with open('parameter/test_prepare.pkl', mode = 'wb') as f:\n",
    "    pickle.dump([X_test, y_test], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e870b04",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Seleção de variáveis através do Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4970ce1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:52.488426Z",
     "start_time": "2022-11-25T17:50:52.486684Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# with open('parameter/train_prepare.pkl', 'rb') as f:\n",
    "#     X_train, y_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4106002a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:52.496985Z",
     "start_time": "2022-11-25T17:50:52.489639Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # training and test dataset for boruta\n",
    "# x_train_n = X_train.drop(['date', 'sales'], axis=1).values\n",
    "# y_train_n = y_train.values.ravel()\n",
    "\n",
    "# # define RadomForestRegressor\n",
    "# rf = RandomForestRegressor( n_jobs= -1 ) # warm_start = True,\n",
    "\n",
    "# # define boruta\n",
    "# boruta = BorutaPy( rf, n_estimators='auto', verbose=2, random_state=42 ).fit( x_train_n, y_train_n )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f276a689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:52.503266Z",
     "start_time": "2022-11-25T17:50:52.500476Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cols_selected = boruta.support_.tolist()\n",
    "\n",
    "# # best features\n",
    "# x_train_fs = X_train.drop(['date', 'sales'], axis=1)\n",
    "# cols_selected_boruta = x_train_fs.iloc[:, cols_selected].columns.to_list()\n",
    "\n",
    "\n",
    "# # not selected boruta\n",
    "# cols_not_selected_boruta = list(np.setdiff1d(x_train_fs.columns, cols_selected_boruta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e41c8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:52.507779Z",
     "start_time": "2022-11-25T17:50:52.505410Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cols_selected_boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620a10ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:37:43.166187Z",
     "start_time": "2022-11-30T17:37:43.159790Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols_selected_boruta = ['store',\n",
    " 'promo',\n",
    " 'store_type',\n",
    " 'assortment',\n",
    " 'competition_distance',\n",
    " 'competition_open_since_month',\n",
    " 'competition_open_since_year',\n",
    " 'promo2',\n",
    " 'promo2_since_week',\n",
    " 'promo2_since_year',\n",
    " 'competition_time_month',\n",
    " 'promo_time_week',\n",
    " 'day_of_week_sin',\n",
    " 'day_of_week_cos',\n",
    " 'month_sin',\n",
    " 'month_cos',\n",
    " 'day_sin',\n",
    " 'day_cos',\n",
    " 'week_of_year_sin',\n",
    " 'week_of_year_cos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['date', 'sales']\n",
    "\n",
    "# final feature\n",
    "cols_selected_boruta_full = cols_selected_boruta.copy()\n",
    "cols_selected_boruta_full.extend( feat_to_add )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413b5d65",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### MACHINE LEARNING MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5c8fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:37:17.580527Z",
     "start_time": "2022-11-30T17:37:17.440393Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('parameter/train_prepare.pkl', 'rb') as f:\n",
    "    X_train, y_train = pickle.load(f)\n",
    "    \n",
    "with open('parameter/test_prepare.pkl', 'rb') as f:\n",
    "    X_test, y_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb2e944",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:37:19.138731Z",
     "start_time": "2022-11-30T17:37:19.109771Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc5f151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:37:47.124548Z",
     "start_time": "2022-11-30T17:37:46.640832Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_train = X_train[cols_selected_boruta]\n",
    "x_test = X_test[cols_selected_boruta]\n",
    "\n",
    "# Tiem Series Data Preparation\n",
    "x_training = X_train[cols_selected_boruta_full]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e89d1e",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 7.1 Average Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debf0678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:37:55.192618Z",
     "start_time": "2022-11-30T17:37:55.136274Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "aux1 = x_test.copy()\n",
    "aux1['sales'] = y_test.copy()\n",
    "\n",
    "# prediction\n",
    "aux2 = aux1[['store', 'sales']].groupby('store').mean().reset_index().rename(columns={'sales': 'predictions'})\n",
    "aux1 = pd.merge(aux1, aux2, how='left', on='store')\n",
    "yhat_baseline = aux1['predictions']\n",
    "\n",
    "# performance\n",
    "baseline_result = ml_error('Average Model', np.expm1( y_test ), np.expm1( yhat_baseline ) )\n",
    "baseline_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae0f54a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 7.2 Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632c009f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:37:57.796001Z",
     "start_time": "2022-11-30T17:37:56.904380Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "lr = LinearRegression().fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_lr = lr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lr_result = ml_error('Linear Regression', np.expm1(y_test), np.expm1(yhat_lr))\n",
    "lr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87119971",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 7.2.1 Linear Regression Model - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5b43b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:50:53.986189Z",
     "start_time": "2022-11-25T17:50:53.811030Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fabc8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:38:01.762890Z",
     "start_time": "2022-11-30T17:38:01.023232Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_result_cv = cross_validation( x_training, 5, 'Linear Regression', lr, verbose=False)\n",
    "lr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfcd463",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 7.3 Linear Regression Regularized Model - Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e3d50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:38:04.878858Z",
     "start_time": "2022-11-30T17:38:04.484683Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "lrr = Lasso(alpha=0.01).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_lrr = lrr.predict(x_test)\n",
    "\n",
    "# performance\n",
    "lrr_result = ml_error('Linear Regression - Lasso', np.expm1(y_test), np.expm1(yhat_lrr))\n",
    "lrr_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f8cec7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 7.3.1 Lasso - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eec168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:38:07.983095Z",
     "start_time": "2022-11-30T17:38:07.666622Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lrr_result_cv = cross_validation( x_training, 5, 'Lasso', lrr, verbose=False)\n",
    "lrr_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8720644f",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 7.4 Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d48048",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:40:44.177436Z",
     "start_time": "2022-11-30T17:38:11.646511Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "rf = RandomForestRegressor(n_estimators=100, n_jobs=-1, random_state=42).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_rf = rf.predict(x_test)\n",
    "\n",
    "# performance\n",
    "rf_result = ml_error('Random Forest Regression', np.expm1(y_test), np.expm1(yhat_rf))\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b18c78",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 7.4.1 Random Forest Regressor- Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9fcef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:42:31.077880Z",
     "start_time": "2022-11-30T17:40:44.280817Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rf_result_cv = cross_validation( x_training, 5, 'Random Forest Regression', rf, verbose=True)\n",
    "rf_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e27008",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 7.5 XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23338c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T17:53:19.351898Z",
     "start_time": "2022-11-30T17:42:31.084484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                             n_estimators=1000, \n",
    "                             eta=0.01,\n",
    "                             max_depth=10,\n",
    "                             subsample=0.7,\n",
    "                             colsample_bytee=0.9).fit(x_train, y_train)\n",
    "\n",
    "# prediction\n",
    "yhat_xgb = model_xgb.predict(x_test)\n",
    "\n",
    "# performance\n",
    "xgb_result = ml_error('XGBoost Regression', np.expm1(y_test), np.expm1(yhat_xgb))\n",
    "xgb_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811daa0e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 7.5.1 XGBoots Regressor - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff2f38d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:01:31.208148Z",
     "start_time": "2022-11-30T17:53:19.358095Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "xgb_result_cv = cross_validation( x_training, 5, 'XGBoots', model_xgb, verbose=True)\n",
    "xgb_result_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ac525c",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 7.5 Compare Model's Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba96fccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:01:31.231046Z",
     "start_time": "2022-11-30T18:01:31.209647Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modelling_result = pd.concat([baseline_result, lr_result, lrr_result, rf_result, xgb_result])\n",
    "modelling_result.sort_values('RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7781fbb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### 7.6.2 Real Performace - Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4584456c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:01:31.240652Z",
     "start_time": "2022-11-30T18:01:31.233645Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "modelling_result = pd.concat([lr_result_cv, lrr_result_cv, rf_result_cv, xgb_result_cv])\n",
    "modelling_result.sort_values('RMSE CV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8cb3435",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31558fb0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Implementação do Random Search do modelo XGBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9062a4",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 8.1 Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58b8743",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T17:56:13.087582Z",
     "start_time": "2022-11-25T17:56:13.082967Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param = {\n",
    "        'n_estimators': [1500, 1700, 2500, 3000, 3500],\n",
    "        'eta': [0.01, 0.03],\n",
    "        'max_depth': [3, 5, 9],\n",
    "        'subsample': [0.1, 0.5, 0.7],\n",
    "        'colsample_bytree': [0.3, 0.7, 0.9],\n",
    "        'min_child_weight': [3, 8, 15] }\n",
    "\n",
    "MAX_EVAL = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2078cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T18:17:51.387054Z",
     "start_time": "2022-11-25T17:56:13.091010Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "final_result = pd.DataFrame()\n",
    "\n",
    "for i in range( MAX_EVAL ):\n",
    "\n",
    "    # choose values for parameters randomly\n",
    "\n",
    "    hp = { k: random.sample( v, 1 )[0] for k, v in param.items() }\n",
    "\n",
    "    print( hp )\n",
    "\n",
    "\n",
    "    # model\n",
    "\n",
    "    model_xgb = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                n_estimators=hp['n_estimators'],\n",
    "                                eta=hp['eta'],\n",
    "                                max_depth=hp['max_depth'],\n",
    "                                subsample=hp['subsample'],\n",
    "                                colsample_bytee=hp['colsample_bytree'],\n",
    "                                min_child_weight=hp['min_child_weight'] )\n",
    "\n",
    "    # performance\n",
    "    result = cross_validation( x_training, 5, 'XGBoost Regressor', model_xgb, verbose=True )\n",
    "\n",
    "    final_result = pd.concat( [final_result, result] )\n",
    "\n",
    "final_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58c3f07",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 8.2 Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd2d803",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T18:17:58.831863Z",
     "start_time": "2022-11-25T18:17:58.828798Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param_tuned = {\n",
    "          'n_estimators': 3500,\n",
    "          'eta': 0.03,\n",
    "          'max_depth': 9,\n",
    "          'subsample': 0.1,\n",
    "          'colsample_bytree': 0.7,\n",
    "          'min_child_weight': 3\n",
    "}\n",
    "\n",
    "# {'n_estimators': 3500, 'eta': 0.03, 'max_depth': 9, 'subsample': 0.1, 'colsample_bytree': 0.7, 'min_child_weight': 3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b6b544",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T18:38:46.198785Z",
     "start_time": "2022-11-25T18:18:45.255599Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model_xgb_tuned = xgb.XGBRegressor( objective='reg:squarederror',\n",
    "                                    n_estimators=param_tuned['n_estimators'],\n",
    "                                    eta=param_tuned['eta'],\n",
    "                                    max_depth=param_tuned['max_depth'],\n",
    "                                    subsample=param_tuned['subsample'],\n",
    "                                    colsample_bytee=param_tuned['colsample_bytree'],\n",
    "                                    min_child_weight=param_tuned['min_child_weight'] ).fit( x_train, y_train )\n",
    "\n",
    "# prediction\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict( x_test )\n",
    "\n",
    "# performance\n",
    "xgb_result_tuned = ml_error( 'XGBoost Regressor', np.expm1( y_test ), np.expm1(yhat_xgb_tuned ) )\n",
    "xgb_result_tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044a3865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T18:17:51.394892Z",
     "start_time": "2022-11-25T18:17:51.394878Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Para salvar o modelo, que vai levar + de 1 dia.\n",
    "\n",
    "pickle.dump(model_xgb_tuned, open('model/model_rossmann.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a2dc44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:22:21.256458Z",
     "start_time": "2022-11-30T18:22:21.039134Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('model/model_rossmann.pkl', 'rb') as f:\n",
    "    model_xgb_tuned = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c15869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:22:42.358424Z",
     "start_time": "2022-11-30T18:22:41.378116Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "yhat_xgb_tuned = model_xgb_tuned.predict( x_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5f009",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Interpretação do ERRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cb1a67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:23:33.522256Z",
     "start_time": "2022-11-30T18:23:33.472117Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df9 = X_test[cols_selected_boruta_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8762de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:23:34.778782Z",
     "start_time": "2022-11-30T18:23:34.759975Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# rescale\n",
    "df9['sales'] = np.expm1(df9['sales'])\n",
    "df9['predictions'] = np.expm1(yhat_xgb_tuned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b935c2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Business Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ca013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:23:43.863857Z",
     "start_time": "2022-11-30T18:23:43.337252Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sum of prediction\n",
    "df91 = df9[['store', 'predictions']].groupby('store').sum().reset_index()\n",
    "\n",
    "# MAE and MAPE\n",
    "# parametro sale com predictions\n",
    "df9_aux1 = df9[['store', 'sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_error(x['sales'], x['predictions'])).reset_index().rename(columns={0 : 'MAE'})\n",
    "\n",
    "df9_aux2 = df9[['store', 'sales', 'predictions']].groupby('store').apply(lambda x: mean_absolute_percentage_error(x['sales'], x['predictions'])).reset_index().rename(columns={0 : 'MAPE'})\n",
    "\n",
    "# merge\n",
    "df9_aux3 = pd.merge(df9_aux1, df9_aux2, how='inner', on='store')\n",
    "df92 = pd.merge(df91, df9_aux3, how='inner', on='store')\n",
    "\n",
    "# Scenarios\n",
    "df92['worst_scenario'] = df92['predictions'] - df92['MAE']\n",
    "df92['best_scenario'] = df92['predictions'] + df92['MAE']\n",
    "\n",
    "# Order columns\n",
    "df92 = df92[['store', 'predictions', 'worst_scenario', 'best_scenario', 'MAE', 'MAPE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91d6990",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:26:19.588008Z",
     "start_time": "2022-11-30T18:26:19.580175Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df92.sort_values(by='MAPE',ascending=True ).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c514e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:23:48.128890Z",
     "start_time": "2022-11-30T18:23:47.742984Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# como observado, existe loja que tem a porcentagem de error maior que 50%, logo fica dificil obter a sua predição de vendas de 6 semanas.\n",
    "sns.scatterplot(x='store', y='MAPE', data=df92);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb73bd09",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Total Performace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e998bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-30T18:23:52.183552Z",
     "start_time": "2022-11-30T18:23:52.170573Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# mostrar a soma de todos os cenários de todas as lojas\n",
    "df93 = df92[['predictions', 'worst_scenario', 'best_scenario']].apply(lambda x: np.sum(x), axis=0).reset_index().rename(columns={'index': 'scenario', 0: 'Values'})\n",
    "df93['Values'] = df93['Values'].map('R${:,.2f}'.format)\n",
    "df93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c42d9d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Deploy Model to Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb9572",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-10-14T11:29:19.732295Z",
     "start_time": "2022-10-14T11:29:19.580324Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Para salvar o modelo, que vai levar + de 1 dia.\n",
    "\n",
    "pickle.dump(model_xgb_tuned, open('/home/caroline/repos/DataScience_em_Producao/model/model_rossmann.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875b149",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Rossamann Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092b672c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T20:51:02.006834Z",
     "start_time": "2022-11-25T20:51:01.984108Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import inflection\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Rossmann(object):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.home_path='/home/caroline/repos/rossmann_sales_project/'\n",
    "        \n",
    "        self.competition_distance_scaler = pickle.load(\n",
    "                 open(self.home_path + 'parameter/competition_distance_scaler.pkl', 'rb'))\n",
    "        \n",
    "        self.competition_time_month_scaler = pickle.load(\n",
    "                open(self.home_path + 'parameter/competition_time_month_scaler.pkl', 'rb'))\n",
    "        \n",
    "        self.promo_time_week_scaler = pickle.load(\n",
    "                open(self.home_path + 'parameter/promo_time_week_scaler.pkl', 'rb'))\n",
    "        \n",
    "        self.year_scaler = pickle.load(\n",
    "                open(self.home_path + 'parameter/year_scaler.pkl', 'rb'))\n",
    "        \n",
    "        self.store_type_scaler = pickle.load(\n",
    "                open(self.home_path + 'parameter/store_type_scaler.pkl', 'rb'))\n",
    "    \n",
    "    def data_cleaning(self, df1):\n",
    "\n",
    "        ## 1.1 Rename Columns\n",
    "\n",
    "        cols_old = ['Store', 'DayOfWeek', 'Date', 'Open', 'Promo',\n",
    "               'StateHoliday', 'SchoolHoliday', 'StoreType', 'Assortment',\n",
    "               'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "               'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek',\n",
    "               'Promo2SinceYear', 'PromoInterval']\n",
    "\n",
    "        snakecase = lambda x: inflection.underscore(x)\n",
    "\n",
    "        cols_new = list(map(snakecase, cols_old))\n",
    "\n",
    "        # rename columns\n",
    "        df1.columns = cols_new\n",
    "\n",
    "\n",
    "        ## 1.3 Data Types\n",
    "\n",
    "        # mudar o types da coluna date\n",
    "        df1['date'] = pd.to_datetime(df1['date'])\n",
    "\n",
    "        ## 1.5 Fillout NA\n",
    "\n",
    "        #competition_distance - distancia em metros da loja competidora proxima\n",
    "            # uma das formas de eliminar os NAs é preenchendo as linhas faltantes\n",
    "            # utilizando a lógica de que talvez  NA é pq a loja competidora está bem distante\n",
    "            # e para isso irei preencher um valor qualquer 200000.0 (maior q o valor maximo do meu dataframe) \n",
    "\n",
    "        df1['competition_distance'] = df1['competition_distance'].apply(lambda x: 200000.0 if math.isnan(x) else x)\n",
    "\n",
    "        #competition_open_since_month - mes/ano que a loja competidora foi aberta\n",
    "            # quando é mais de uma coluna coloca axis=1\n",
    "\n",
    "        df1['competition_open_since_month']= df1.apply(lambda x: x['date'].month \n",
    "                                                       if math.isnan(x['competition_open_since_month']) else \n",
    "                                                       x['competition_open_since_month'], axis=1)\n",
    "\n",
    "        #competition_open_since_year  \n",
    "        df1['competition_open_since_year']= df1.apply(lambda x: x['date'].year \n",
    "                                                       if math.isnan(x['competition_open_since_year']) else      \n",
    "                                                       x['competition_open_since_year'], axis=1)                       \n",
    "        #promo2_since_week\n",
    "        df1['promo2_since_week']= df1.apply(lambda x: x['date'].week \n",
    "                                                       if math.isnan(x['promo2_since_week']) else      \n",
    "                                                       x['promo2_since_week'], axis=1)  \n",
    "        #promo2_since_year   \n",
    "        df1['promo2_since_year']= df1.apply(lambda x: x['date'].year \n",
    "                                                       if math.isnan(x['promo2_since_year']) else      \n",
    "                                                       x['promo2_since_year'], axis=1)\n",
    "        #promo_interval - intervalos consecutivos quando a promo2 foi iniciada\n",
    "        # dicionario indicando o numero dos meses\n",
    "        month_map = {1: 'Jan',\n",
    "                    2: 'Feb',\n",
    "                    3: 'Mar',\n",
    "                    4: 'Apr',\n",
    "                    5: 'May',\n",
    "                    6: 'Jun',\n",
    "                    7: 'Jul',\n",
    "                    8: 'Aug',\n",
    "                    9: 'Sept',\n",
    "                    10: 'Oct',\n",
    "                    11: 'Nov',\n",
    "                    12: 'Dec'}\n",
    "        df1['promo_interval'] = df1['promo_interval'].fillna(0)\n",
    "            # assumption: criadno uma coluna onde indica o mes da coluna date     \n",
    "        df1['month_map'] = df1['date'].dt.month.map(month_map)\n",
    "\n",
    "            # assumption: se o mes estiver presente na coluna 'promo_interval' logo tem promocao ativa naquela data \n",
    "        df1['is_promo'] = df1[['promo_interval', 'month_map']].apply(lambda x: 0 if x['promo_interval']==0 else \n",
    "                                                                    1 if x['month_map'] in x['promo_interval'].split(',') \n",
    "                                                                    else 0, axis=1)\n",
    "\n",
    "\n",
    "        ## 1.6 Change Types\n",
    "\n",
    "        df1['competition_open_since_month'] = df1['competition_open_since_month'].astype(int)\n",
    "        df1['competition_open_since_year'] = df1['competition_open_since_year'].astype(int)\n",
    "\n",
    "        df1['promo2_since_week'] = df1['promo2_since_week'].astype(int)\n",
    "        df1['promo2_since_year'] = df1['promo2_since_year'].astype(int)\n",
    "\n",
    "        \n",
    "        return df1\n",
    "    \n",
    "\n",
    "    \n",
    "    def feature_engineering(self, df2):\n",
    "\n",
    "        ## extraindo novas features da coluna date\n",
    "        # year\n",
    "        df2['year'] = df2['date'].dt.year\n",
    "\n",
    "        # month\n",
    "        df2['month'] = df2['date'].dt.month\n",
    "\n",
    "        # day\n",
    "        df2['day'] = df2['date'].dt.day\n",
    "\n",
    "        # week of year\n",
    "        df2['week_of_year'] = df2['date'].dt.isocalendar().week\n",
    "\n",
    "        # year week\n",
    "        df2['year_week'] = df2['date'].dt.strftime('%Y-%W')\n",
    "\n",
    "        ## features de competition\n",
    "        # competition since - juntas o mes - ano\n",
    "        df2['competition_since'] = df2.apply(lambda x: datetime.datetime(year= x['competition_open_since_year'], \n",
    "                                                                         month= x['competition_open_since_month'] , day=1), axis=1)\n",
    "        df2['competition_time_month'] = ((df2['date'] - df2['competition_since'])/30).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "        ## feature de promo\n",
    "        # promo since\n",
    "        df2['promo_since'] = df2['promo2_since_year'].astype(str) + '-' + df2['promo2_since_week'].astype(str)\n",
    "        # transformando em datetime\n",
    "        df2['promo_since'] = df2['promo_since'].apply(\n",
    "                                lambda x: datetime.datetime.strptime(x + '-1', '%Y-%W-%w') - datetime.timedelta(days=7))\n",
    "        df2['promo_time_week'] = ((df2['date'] - df2['promo_since'])/7).apply(lambda x: x.days).astype(int)\n",
    "\n",
    "        ## feature assortment\n",
    "        df2['assortment']= df2['assortment'].apply(lambda x: 'basic' if x == 'a' else 'extra' if x == 'b' else 'extended')\n",
    "\n",
    "        # state holiday\n",
    "        df2['state_holiday'] = df2['state_holiday'].apply(lambda x:'public_holiday' if x == 'a' else \n",
    "                                                          'easter_holiday' if x == 'b' else \n",
    "                                                          'christmas' if x == 'c' else 'regular_day') \n",
    "\n",
    "        # 3.0 - Filtragem das Variáveis\n",
    "        ## 3.1 - Filtragem das linhas\n",
    "\n",
    "        # as restrições observadas para o negócio foram loja fechada e vendas nulas\n",
    "        df2 = df2[(df2['open'] != 0)]\n",
    "\n",
    "        ## 3.2 - Seleção\n",
    "\n",
    "        # a coluna customers é uma restrição para o modelo, pois tem essa informação daqui 6 semanas\n",
    "        # a não ser que eu faça outro modelo de previsão para calcular a coluna customers daqui 6 semanas.\n",
    "        # então por isso, foi retirar essa coluna do meu dataset para fazer a predição de vendas daqui 6 semanas.\n",
    "        cols_drop = [ 'open', 'promo_interval', 'month_map']\n",
    "        df2 = df2.drop(cols_drop, axis=1)\n",
    "        \n",
    "        return df2\n",
    "    \n",
    "    def data_preparation(self, df5):\n",
    "\n",
    "        ## 5.2 Rescaling\n",
    "        # competition distance\n",
    "        df5['competition_distance'] = self.competition_distance_scaler.transform( df5[['competition_distance']].values )\n",
    "\n",
    "\n",
    "        # competition time month\n",
    "        df5['competition_time_month'] = self.competition_time_month_scaler.transform( df5[['competition_time_month']].values )\n",
    "        \n",
    "\n",
    "        # promo time week\n",
    "        df5['promo_time_week'] = self.promo_time_week_scaler.transform( df5[['promo_time_week']].values )\n",
    "        \n",
    "\n",
    "        # year\n",
    "        df5['year'] = self.year_scaler.transform( df5[['year']].values )\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        ## 5.3 Transformação\n",
    "\n",
    "        ### 5.3.1 Enconding \n",
    "\n",
    "        # enconding - muda a variavel de categorica para numerica sem mudar o conteudo de unformação\n",
    "\n",
    "        # state_holiday - One Hot Encoding\n",
    "        df5 = pd.get_dummies(df5, prefix=['state_holiday'], columns=['state_holiday'])\n",
    "\n",
    "        # store_type - Label Enconding\n",
    "        df5['store_type'] = self.store_type_scaler.transform(df5['store_type'])\n",
    "\n",
    "\n",
    "        # assortment - Ordinal Enconding\n",
    "        assortment_dict = {'basic': 1, 'extra': 2, 'extended': 3}\n",
    "        df5['assortment'] = df5['assortment'].map(assortment_dict)\n",
    "        \n",
    "        ## natureza ciclica - seno e cosseno\n",
    "        # day of week\n",
    "        df5['day_of_week_sin'] = df5['day_of_week'].apply(lambda x: np.sin(x * ( 2. * np.pi/7 )))\n",
    "        df5['day_of_week_cos'] = df5['day_of_week'].apply(lambda x: np.cos(x * ( 2. * np.pi/7 )))\n",
    "\n",
    "        # month\n",
    "        df5['month_sin'] = df5['month'].apply(lambda x: np.sin(x * ( 2. * np.pi/12 )))\n",
    "        df5['month_cos'] = df5['month'].apply(lambda x: np.cos(x * ( 2. * np.pi/12 )))\n",
    "\n",
    "        # day\n",
    "        df5['day_sin'] = df5['day'].apply(lambda x: np.sin(x * ( 2. * np.pi/30 )))\n",
    "        df5['day_cos'] = df5['day'].apply(lambda x: np.cos(x * ( 2. * np.pi/30 )))\n",
    "\n",
    "        # week of year\n",
    "        df5['week_of_year_sin'] = df5['week_of_year'].apply(lambda x: np.sin(x * ( 2. * np.pi/52 )))\n",
    "        df5['week_of_year_cos'] = df5['week_of_year'].apply(lambda x: np.cos(x * ( 2. * np.pi/52 )))\n",
    "        \n",
    "        cols_selected = ['store', 'promo', 'store_type', 'assortment', 'competition_distance', 'competition_open_since_month',\n",
    "                                'competition_open_since_year', 'promo2', 'promo2_since_week', 'promo2_since_year', \n",
    "                                'competition_time_month', 'promo_time_week', 'day_of_week_sin', 'day_of_week_cos', \n",
    "                                'month_sin', 'month_cos', 'day_sin', 'day_cos', 'week_of_year_sin', 'week_of_year_cos']\n",
    "        \n",
    "        return df5[cols_selected]\n",
    "    \n",
    "    def get_prediction(self, model, original_data, test_data):\n",
    "        # prediction\n",
    "        pred = model.predict(test_data)\n",
    "        \n",
    "        # join pred into the original data\n",
    "        original_data['prediction'] = np.expm1(pred)\n",
    "        \n",
    "        return original_data.to_json(orient='records', date_format='iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a067f6b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cbfeb63",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### API Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8643118e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-25T20:51:06.058331Z",
     "start_time": "2022-11-25T20:51:05.916299Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from flask import Flask, request, Response\n",
    "from rossmann.Rossmann import Rossmann\n",
    "\n",
    "# loading model\n",
    "model = pickle.load(open('model/model_rossmann.pkl', 'rb'))\n",
    "\n",
    "# initialize API\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/rossmann/predict', methods=['POST'])\n",
    "\n",
    "def rossmann_predict():\n",
    "    test_json = request.get_json()\n",
    "    \n",
    "    if test_json: # there is data\n",
    "        if isinstance(test_json, dict): # unique example\n",
    "            test_raw = pd.DataFrame(test_json, index=[0])\n",
    "            \n",
    "        else: # multiple example\n",
    "            test_raw = pd.DataFrame(test_json, columns= test_json[0].keys())\n",
    "            \n",
    "        # Instantiate Rossmann class\n",
    "        pipeline = Rossmann()\n",
    "        \n",
    "        # data cleaning\n",
    "        df1 = pipeline.data_cleaning(test_raw)\n",
    "        \n",
    "        # feature engineering\n",
    "        df2 = pipeline.feature_engineering(df1)\n",
    "        \n",
    "        # data preparation\n",
    "        df3 = pipeline.data_preparation(df2)\n",
    "        \n",
    "        # prediction\n",
    "        df_response = pipeline.get_prediction(model, test_raw, df3)\n",
    "        \n",
    "        return df_response\n",
    "    \n",
    "    else: \n",
    "        return Response('{}', status=200, mimetype='application/json')\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run('0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fae76",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e86a2a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### API Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84f81e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T19:32:40.350090Z",
     "start_time": "2022-11-27T19:32:40.281712Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7f4dbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:05.673890Z",
     "start_time": "2022-11-27T21:58:05.639406Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# loading test dataset\n",
    "df10 = pd.read_csv('/home/caroline/repos/DataScience_em_Producao/data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5eb81d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:06.057388Z",
     "start_time": "2022-11-27T21:58:06.034690Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# merge test dataset + store\n",
    "df_test = pd.merge(df10, df_store_raw, how='left', on = 'Store')\n",
    "\n",
    "# choose store for prediction\n",
    "df_test = df_test[df_test['Store'].isin([58,50,23])]\n",
    "\n",
    "# remove closed days\n",
    "df_test = df_test[df_test['Open'] != 0]\n",
    "df_test = df_test[~df_test['Open'].isnull()]\n",
    "df_test = df_test.drop('Id', axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb41bb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:06.636011Z",
     "start_time": "2022-11-27T21:58:06.627432Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# convert dataframe to json\n",
    "data = json.dumps(df_test.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6836b9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:09.373590Z",
     "start_time": "2022-11-27T21:58:07.404747Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# API call\n",
    "# url = 'http://0.0.0.0:5000/rossmann/predict' #/rossman/predict é o endpoint em ambiente local\n",
    "url = 'https://teste-deploy-render-bc3n.onrender.com/rossmann/predict'\n",
    "\n",
    "header = {'Content-type': 'application/json'} # indica qual tipo de dado esta recebendo\n",
    "data = data\n",
    "\n",
    "r = requests.post( url, data = data, headers = header)\n",
    "# metodo POST serve para enviar o dado\n",
    "\n",
    "print(f'Status code {r.status_code}')\n",
    "# para indicar se a request é válida\n",
    "# reorno de 200 significa que está tudo okay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10c69ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:10.460968Z",
     "start_time": "2022-11-27T21:58:10.451069Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d1 = pd.DataFrame(r.json(), columns=r.json()[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6c6a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:11.646603Z",
     "start_time": "2022-11-27T21:58:11.639097Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d2 = d1[['store', 'prediction']].groupby('store').sum().reset_index()\n",
    "\n",
    "for i in range(len(d2)):\n",
    "    print('Store Number {} will sell R${:,.2f} in the next 6 weeks'.format(\n",
    "                        d2.loc[i, 'store'],\n",
    "                        d2.loc[i, 'prediction'] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892beea4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:12.811860Z",
     "start_time": "2022-11-27T21:58:12.803514Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_plot = d1[['store', 'prediction', 'year_week']].groupby(['store', 'year_week']).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296df7a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:13.600033Z",
     "start_time": "2022-11-27T21:58:13.589268Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "d_plot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e141930",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-27T21:58:14.631867Z",
     "start_time": "2022-11-27T21:58:14.435671Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots()\n",
    "f = plt.figure(figsize=(20,8))\n",
    "ax = sns.lineplot(data= d_plot, x= 'year_week', y= 'prediction', hue='store', marker='o')\n",
    "\n",
    "# for v in d_plot.iterrows():\n",
    "#     ax.text(v[1][0] , v[1][2], f'{round(v[1][2]/1000000, 2)} M', \n",
    "#             horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "# ax.ticklabel_format(style='plain', axis=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f5328",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8becd875",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSEmProducao",
   "language": "python",
   "name": "dsemproducao"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
